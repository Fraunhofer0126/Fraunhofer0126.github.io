<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>矩阵分析-最优化理论笔记</title>
    <url>/2020/11/10/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<a id="more"></a>

<h1 id="数学知识"><a href="#数学知识" class="headerlink" title="数学知识"></a>数学知识</h1><ul>
<li>梯度 $grad f(x_0,y_0) = f_x(x_0,y_0)i + f_y(x_0,y_0)j$</li>
<li>方向导数 $\frac{\partial f}{\partial l} |_{(x_0,y_0)} = e_l^T * gradf(x_0,y_0) = |gradf(x_0,y_0)|cos\theta$, </br><br>其中$e_l = (cos\alpha, cos\beta)$,是和$l$同方向的单位向量</li>
<li>由上可知，$|\theta| \leq 90$时，方向导数，也就是函数沿着$e_l$方向的变化量为正, 函数的变化量是增加的。</br><br>于是，梯度方向是函数<strong>增加</strong>最快的方向</li>
<li>向量函数的Taylor展开:</br><br>$F(x)=F(x_\theta)+\Delta F(x)^T | _{x=x_\theta}(x- x_\theta)+ \frac{1}{2}(x- x_\theta)^T\Delta^2F(x)| _{x = x_\theta}(x - x_\theta)+…$ </br></li>
<li>设$A$是$n$阶实对称矩阵，记$\lambda_{max}, \lambda_{min}$分别是$A$中所有特征值中的最大值与最小值，则$\lambda_{max}X^TX \leq X^TAX \leq \lambda_{min}X^TX$</br><br>当$||x||_2 =1$ 时函数$F(x)= x^TAx$ 最大值是 $\lambda _{max}$， 最小值是 $\lambda _{min}$.</li>
</ul>
<h2 id="Hessian矩阵相关"><a href="#Hessian矩阵相关" class="headerlink" title="Hessian矩阵相关"></a>Hessian矩阵相关</h2><ol>
<li>若Hessian阵特征值全为正，则局部有极小值</li>
<li>若Hessian阵特征值全为负，则局部有极大值</li>
<li>若Hessian阵特征值有正有负，则局部有鞍点（$x$是$f$一个横截面的局部极大点，却是f另一个横截面的局部极小点）</li>
<li>若Hessian阵特征值有零，也是不确定的，$x$可以是鞍点或者<strong>平稳谷</strong>的一部分 </li>
</ol>
<ul>
<li>证明–对Hessian矩阵$A$做正交对角化$x=By$, $B$恰好正交对角化$A$, 且有$B^T=B^{-1}$</br><br>$x^TAx = y^T\Lambda y = \lambda_1y_1^2+\lambda_2y_2^2+…+\lambda_ny_n^2$</br><br>而二阶梯度种二次型的值就取决于Hessian阵的各个特征值是否同时同号</br></li>
</ul>
<ol start="5">
<li><p>$F(x)$是二阶连续可导函数时，其混合偏导相等，Hessian矩阵是对称矩阵</br><br>对称矩阵的特征向量正交，因此Hessian矩阵的特征向量正交</p>
</li>
<li><p>若$F(x)$为二次函数，$\Delta^2F(x)$就是它的Hessian矩阵</p>
</li>
<li><p>$F(x)$在其Hessian矩阵较大特征值对应的特征方向最陡峭，</br><br>$F(x)$的曲率为$\frac{p^T\Delta^2F(x)p}{||p||_2^2}, p$为某方向（待求的曲率方向）对于分子项，可根据<strong>数学知识</strong>得，最大值在$F(x)$的Hessian矩阵特征值大的对应的特征向量方向取，最小值同理。 </p>
</li>
</ol>
<h1 id="下降法"><a href="#下降法" class="headerlink" title="下降法"></a>下降法</h1><h2 id="梯度下降-只利用了一阶梯度信息"><a href="#梯度下降-只利用了一阶梯度信息" class="headerlink" title="梯度下降 只利用了一阶梯度信息"></a><strong>梯度下降</strong> 只利用了一阶梯度信息</br></h2><ul>
<li>基本思想是$\Delta x_k = x_{k+1}-x_k = \alpha_k p_k$, 即$x_{k+1} =x_k+\alpha_k p_k$其中$p_k$是搜索 方向, $\alpha_k$ 是学习率。</br><br>$F(x)$一阶近似$F(x_{k+1}) = F(x_k+\Delta x_k) \approx F(x_k) +g_k^T \Delta x_k$ </br><br>选择$p_k = -g_k$ </br><br>有:<strong>$x_{k+1} = x_k-\alpha_k g_k$</strong></br></li>
</ul>
<ul>
<li><p><strong>Tips</strong></p>
<ul>
<li><p>二阶展开的稳定学习率：$\alpha &lt; \frac{2}{\lambda_{max}}$, 其中$\lambda_{max}$是Hessian矩阵的最大特征值</p>
</li>
<li><p>$\alpha_k$可以选择固定值，也可以每次都沿直线最小化$\alpha_k = \frac{g_k^T g_k}{g_k^T A_k g_k}$, </br><br>因为对于$F(x_k-\alpha g_k)=F(x_\theta)-\alpha g_k^Tg^k+\frac{1}{2}\alpha^2g_k^TA_kg_k$, 二次项太大，梯度下降是有可能向上移动的。当$g^TAg$为负或零，近似的泰勒级数表明增加$\alpha$将永远使$F$下降；而如果$g^TAg$为正，使用上面的优化$\alpha$，带入计算可得，一次项和二次型合并得到一个非正值。</br></p>
</li>
<li><p>推导: $\frac{dF(x_k-\alpha_k g_k)}{d\alpha_k}=-g_k^Tg_k+\alpha_k g_k^T A_k g_k$，(将$x=x_k-\alpha_k g_k, (x-x_k)=-\alpha_k g_k$代入二阶Taylor展开,再对$\alpha_k$求导)</br><br>令其为0即可得$\alpha_k$，即最优步长。</p>
</li>
<li><p>$\frac{dF(x_k-\alpha_k g_k)}{d\alpha_k}=\frac{dF(x_{k+1})}{d\alpha_k}=g_{k+1}^T \frac{d(x_k-\alpha_k g_k)}{d\alpha_k}=-g_{k+1}^T g_k$</br><br>因为直线最小化需要$\frac{d(F(x_k-\alpha_k g_k))}{d\alpha_k}=0$, 故下一次下降方向与这一次正交。</p>
</li>
</ul>
</li>
</ul>
<h2 id="牛顿法-和一阶方法相比，二阶方法使用Hessian矩阵进行优化"><a href="#牛顿法-和一阶方法相比，二阶方法使用Hessian矩阵进行优化" class="headerlink" title="牛顿法 和一阶方法相比，二阶方法使用Hessian矩阵进行优化"></a><strong>牛顿法</strong> 和一阶方法相比，二阶方法使用Hessian矩阵进行优化</br></h2><ul>
<li>牛顿法只适用于Hessian矩阵是正定的情况</li>
<li>$F(x)$二阶近似$F(x_{k+1}) = F(x_k+\Delta x_k) \approx F(x_k) + g_k^T \Delta x_k + \frac{1}{2}\Delta x_k^T A_k \Delta x_k$ </br><br>($A_k$就是Hessian矩阵)</br><br>对这个近似出来的函数求梯度，然后令梯度为0，得到$g_k+A_k \Delta x_k = 0$($A_k$对称)</br><br>于是$\Delta x_k = - A_k^{-1}g_k$</br><br>最后$x_{k+1} = x_k - A_k^{-1}g_k$</li>
</ul>
<h2 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法 "></a><strong>共轭梯度法</strong> </br></h2><ul>
<li><p>根据”直线最小化”，$g_{k+1}^T p_k=0$,$p_k$对$p_{k+1}$没有贡献，</br><br>下降正交方向的选择不会保持前一搜索方向上的最小值。这产生了锯齿形的结果,</br><br>通过遵循每次先搜索结束时的梯度，我们再某种程度上撤销了之前先搜索的方向上取得的进展,</br><br>共轭梯度试图解决这个问题。</p>
</li>
<li><p>针对二次函数$F(x)=\frac{1}{2}x^T A x+d^Tx+c$</br><br>$\Delta F(x)=Ax+d, \Delta^2F(x)=A$</p>
</li>
<li><p>若一组向量$p_k^T,p_j$关于一个正定矩阵A共轭，则$p_k^TAp_j=0, k\neq j$</br><br>根据Hessian矩阵特征向量正交，得Hessian矩阵的特征向量共轭</p>
</li>
<li><p>迭代$k$次梯度变化$\Delta g_k=A\Delta x_k$, 又有$\Delta x_k=\alpha_k p_k$</br><br>于是共轭条件可以改写:</br><br>$p_k^TAp_j=0=\alpha_k p_k^TAp_j=\Delta x_k^TAp_j=(A\Delta x_k)^Tp_j=\Delta g_k^Tp_j, k\neq j$</p>
</li>
<li><p>我们寻求$p_k= -g_k^T+\beta_kp_{k-1}$</br><br><strong>Fletcher-Reeves</strong>: $\beta_K=\frac{g_k^Tg_k}{g_{k-1}^Tg_{k-1}}$</p>
</li>
<li><p>算法步骤</p>
<ol>
<li>根据初始点$x_0$计算初始梯度/方向$p_0$</li>
<li>根据直线最小化$\alpha_k = \frac{g_k^T g_k}{g_k^T A_k g_k}$计算步长$\alpha_0$</li>
<li>根据$x_1=x_0-\alpha_0g_0$更新$x_1$</li>
<li>根据$x_1$带入函数的梯度求出$g_1$</li>
<li>根据Fletcher-Reeves计算$\beta_1$</li>
<li>根据$p_k= -g_k^T+\beta_kp_{k-1}$修正$p_1$</li>
<li>回到第二步求$\alpha_{k+1}$依次循环</li>
</ol>
</li>
</ul>
<h1 id="末尾"><a href="#末尾" class="headerlink" title="末尾"></a>末尾</h1>]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
  </entry>
</search>
